{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install orjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading PyMuPDF-1.23.8-cp311-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting PyMuPDFb==1.23.7 (from PyMuPDF)\n",
      "  Downloading PyMuPDFb-1.23.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Downloading PyMuPDF-1.23.8-cp311-none-manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyMuPDFb-1.23.7-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDFb, PyMuPDF\n",
      "Successfully installed PyMuPDF-1.23.8 PyMuPDFb-1.23.7\n",
      "Requirement already satisfied: Pillow in /home/patidarritesh/miniconda3/envs/venv/lib/python3.11/site-packages (10.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF\n",
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jsonlines\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting attrs>=19.2.0 (from jsonlines)\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m439.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Installing collected packages: attrs, jsonlines\n",
      "Successfully installed attrs-23.1.0 jsonlines-4.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import random\n",
    "from typing import Dict, Tuple, Callable\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from typing import List, Optional\n",
    "import pypdf\n",
    "import orjson\n",
    "import jsonlines     \n",
    "import fitz  # PyMuPDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/NAS/patidarritesh/Pdf_2_LaTeX/pdf_2_tex/dataset/root/pdf/2201.00001.pdf\n",
      "/mnt/NAS/patidarritesh/Pdf_2_LaTeX/pdf_2_tex/dataset/root/pdf/2201.00012.pdf\n",
      "/mnt/NAS/patidarritesh/Pdf_2_LaTeX/pdf_2_tex/dataset/root/pdf/2201.00034.pdf\n",
      "/mnt/NAS/patidarritesh/Pdf_2_LaTeX/pdf_2_tex/dataset/root/pdf/2201.00070.pdf\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "json_path = Path('/mnt/NAS/patidarritesh/Pdf_2_LaTeX/pdf_2_tex/dataset/root/train.jsonl')\n",
    "\n",
    "# Open the JSONL file for reading\n",
    "with jsonlines.open(json_path) as reader:\n",
    "    for line_number, line in enumerate(reader, start=1):\n",
    "        # Process each line as needed\n",
    "        print(line['pdf'])\n",
    "\n",
    "print(line_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "This source code is licensed under the MIT license found in the\n",
    "LICENSE file in the root directory of this source tree.\n",
    "\"\"\"\n",
    "# Implements image augmentation\n",
    "\n",
    "import albumentations as alb\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import numpy as np\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "\n",
    "\n",
    "def alb_wrapper(transform):\n",
    "    def f(im):\n",
    "        return transform(image=np.asarray(im))[\"image\"]\n",
    "\n",
    "    return f\n",
    "\n",
    "\n",
    "class Erosion(alb.ImageOnlyTransform):\n",
    "    \"\"\"\n",
    "    Apply erosion operation to an image.\n",
    "\n",
    "    Erosion is a morphological operation that shrinks the white regions in a binary image.\n",
    "\n",
    "    Args:\n",
    "        scale (int or tuple/list of int): The scale or range for the size of the erosion kernel.\n",
    "            If an integer is provided, a square kernel of that size will be used.\n",
    "            If a tuple or list is provided, it should contain two integers representing the minimum\n",
    "            and maximum sizes for the erosion kernel.\n",
    "        always_apply (bool, optional): Whether to always apply this transformation. Default is False.\n",
    "        p (float, optional): The probability of applying this transformation. Default is 0.5.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The transformed image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, always_apply=False, p=0.5):\n",
    "        super().__init__(always_apply=always_apply, p=p)\n",
    "        if type(scale) is tuple or type(scale) is list:\n",
    "            assert len(scale) == 2\n",
    "            self.scale = scale\n",
    "        else:\n",
    "            self.scale = (scale, scale)\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        kernel = cv2.getStructuringElement(\n",
    "            cv2.MORPH_ELLIPSE, tuple(np.random.randint(self.scale[0], self.scale[1], 2))\n",
    "        )\n",
    "        img = cv2.erode(img, kernel, iterations=1)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Dilation(alb.ImageOnlyTransform):\n",
    "    \"\"\"\n",
    "    Apply dilation operation to an image.\n",
    "\n",
    "    Dilation is a morphological operation that expands the white regions in a binary image.\n",
    "\n",
    "    Args:\n",
    "        scale (int or tuple/list of int): The scale or range for the size of the dilation kernel.\n",
    "            If an integer is provided, a square kernel of that size will be used.\n",
    "            If a tuple or list is provided, it should contain two integers representing the minimum\n",
    "            and maximum sizes for the dilation kernel.\n",
    "        always_apply (bool, optional): Whether to always apply this transformation. Default is False.\n",
    "        p (float, optional): The probability of applying this transformation. Default is 0.5.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The transformed image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, always_apply=False, p=0.5):\n",
    "        super().__init__(always_apply=always_apply, p=p)\n",
    "        if type(scale) is tuple or type(scale) is list:\n",
    "            assert len(scale) == 2\n",
    "            self.scale = scale\n",
    "        else:\n",
    "            self.scale = (scale, scale)\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        kernel = cv2.getStructuringElement(\n",
    "            cv2.MORPH_ELLIPSE, tuple(np.random.randint(self.scale[0], self.scale[1], 2))\n",
    "        )\n",
    "        img = cv2.dilate(img, kernel, iterations=1)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Bitmap(alb.ImageOnlyTransform):\n",
    "    \"\"\"\n",
    "    Apply a bitmap-style transformation to an image.\n",
    "\n",
    "    This transformation replaces all pixel values below a certain threshold with a specified value.\n",
    "\n",
    "    Args:\n",
    "        value (int, optional): The value to replace pixels below the threshold with. Default is 0.\n",
    "        lower (int, optional): The threshold value below which pixels will be replaced. Default is 200.\n",
    "        always_apply (bool, optional): Whether to always apply this transformation. Default is False.\n",
    "        p (float, optional): The probability of applying this transformation. Default is 0.5.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The transformed image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, value=0, lower=200, always_apply=False, p=0.5):\n",
    "        super().__init__(always_apply=always_apply, p=p)\n",
    "        self.lower = lower\n",
    "        self.value = value\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        img = img.copy()\n",
    "        img[img < self.lower] = self.value\n",
    "        return img\n",
    "\n",
    "\n",
    "train_transform = alb_wrapper(\n",
    "    alb.Compose(\n",
    "        [\n",
    "            Bitmap(p=0.05),\n",
    "            alb.OneOf([Erosion((2, 3)), Dilation((2, 3))], p=0.02),\n",
    "            alb.Affine(shear={\"x\": (0, 3), \"y\": (-3, 0)}, cval=(255, 255, 255), p=0.03),\n",
    "            alb.ShiftScaleRotate(\n",
    "                shift_limit_x=(0, 0.04),\n",
    "                shift_limit_y=(0, 0.03),\n",
    "                scale_limit=(-0.15, 0.03),\n",
    "                rotate_limit=2,\n",
    "                border_mode=0,\n",
    "                interpolation=2,\n",
    "                value=(255, 255, 255),\n",
    "                p=0.03,\n",
    "            ),\n",
    "            alb.GridDistortion(\n",
    "                distort_limit=0.05,\n",
    "                border_mode=0,\n",
    "                interpolation=2,\n",
    "                value=(255, 255, 255),\n",
    "                p=0.04,\n",
    "            ),\n",
    "            alb.Compose(\n",
    "                [\n",
    "                    alb.Affine(\n",
    "                        translate_px=(0, 5), always_apply=True, cval=(255, 255, 255)\n",
    "                    ),\n",
    "                    alb.ElasticTransform(\n",
    "                        p=1,\n",
    "                        alpha=50,\n",
    "                        sigma=120 * 0.1,\n",
    "                        alpha_affine=120 * 0.01,\n",
    "                        border_mode=0,\n",
    "                        value=(255, 255, 255),\n",
    "                    ),\n",
    "                ],\n",
    "                p=0.04,\n",
    "            ),\n",
    "            alb.RandomBrightnessContrast(0.1, 0.1, True, p=0.03),\n",
    "            alb.ImageCompression(95, p=0.07),\n",
    "            alb.GaussNoise(20, p=0.08),\n",
    "            alb.GaussianBlur((3, 3), p=0.03),\n",
    "            alb.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "test_transform = alb_wrapper(\n",
    "    alb.Compose(\n",
    "        [\n",
    "            alb.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import albumentations as alb\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# def alb_wrapper(transform):\n",
    "#     def f(im):\n",
    "#         return transform(image=np.asarray(im))[\"image\"]\n",
    "\n",
    "#     return f\n",
    "\n",
    "# train_transform = alb_wrapper(\n",
    "#         alb.Compose(\n",
    "#         [\n",
    "#             alb.Compose(\n",
    "#                 [alb.ShiftScaleRotate(shift_limit=0, scale_limit=(-.15, 0), rotate_limit=1, border_mode=0, interpolation=3,\n",
    "#                                     value=[255, 255, 255], p=1),\n",
    "#                 alb.GridDistortion(distort_limit=0.1, border_mode=0, interpolation=3, value=[255, 255, 255], p=.5)], p=.15),\n",
    "#             # alb.InvertImg(p=.15),\n",
    "#             alb.RGBShift(r_shift_limit=15, g_shift_limit=15,\n",
    "#                         b_shift_limit=15, p=0.3),\n",
    "#             alb.GaussNoise(10, p=.2),\n",
    "#             alb.RandomBrightnessContrast(.05, (-.2, 0), True, p=0.2),\n",
    "#             alb.ImageCompression(95, p=.3),\n",
    "#             alb.ToGray(always_apply=True),\n",
    "#             alb.Normalize((0.7931, 0.7931, 0.7931), (0.1738, 0.1738, 0.1738)),\n",
    "#             # alb.Sharpen()\n",
    "#             ToTensorV2(),\n",
    "            \n",
    "#         ]\n",
    "#     )\n",
    "# )\n",
    "# test_transform = alb.Compose(\n",
    "#     [\n",
    "#         alb.ToGray(always_apply=True),\n",
    "#         alb.Normalize((0.7931, 0.7931, 0.7931), (0.1738, 0.1738, 0.1738)),\n",
    "#         # alb.Sharpen()\n",
    "#         ToTensorV2(),\n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from PIL import ImageOps\n",
    "from timm.models.swin_transformer import SwinTransformer\n",
    "from torchvision.transforms.functional import resize, rotate\n",
    "\n",
    "class enc():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input_size = [4480,672]\n",
    "        self.align_long_axis = True\n",
    "        self.training = True\n",
    "\n",
    "    def crop_margin(self, img: Image.Image) -> Image.Image:\n",
    "        data = np.array(img.convert(\"L\"))\n",
    "        data = data.astype(np.uint8)\n",
    "        max_val = data.max()\n",
    "        min_val = data.min()\n",
    "        if max_val == min_val:\n",
    "            return img\n",
    "        data = (data - min_val) / (max_val - min_val) * 255\n",
    "        gray = 255 * (data < 200).astype(np.uint8)\n",
    "\n",
    "        coords = cv2.findNonZero(gray)  # Find all non-zero points (text)\n",
    "        a, b, w, h = cv2.boundingRect(coords)  # Find minimum spanning bounding box\n",
    "        return img.crop((a, b, w + a, h + b))\n",
    "\n",
    "    # def to_tensor(self, img: Image.Image):\n",
    "    #     if self.training:\n",
    "    #         return train_transform\n",
    "    #     else:\n",
    "    #         return test_transform\n",
    "    def to_tensor(self, img: Image.Image):\n",
    "        if self.training:\n",
    "            # return train_transform(image = img)['image']\n",
    "            return transforms.ToTensor()(img)\n",
    "        else:\n",
    "            return transforms.ToTensor()(img)\n",
    "# \"\"\"\n",
    "#     def prepare_input(self, pdf_path:str, random_padding: bool = False):\n",
    "#         self.input_tensor = None\n",
    "#         if pdf_path is None:\n",
    "#             return\n",
    "#         img_list = []\n",
    "#         # Convert PDF to images using PyMuPDF\n",
    "#         input_size = [896,672]\n",
    "#         doc = fitz.open(pdf_path)\n",
    "#         for page_number in range(len(doc)):\n",
    "#             page = doc[page_number]\n",
    "#             image = page.get_pixmap()\n",
    "#             # Convert Pixmap to PIL Image\n",
    "#             img = Image.frombytes(\"RGB\", [image.width, image.height], image.samples)\n",
    "\n",
    "#             # crop margins\n",
    "#             try:\n",
    "#                 img = self.crop_margin(img.convert(\"RGB\"))\n",
    "#             except OSError:\n",
    "#                 # might throw an error for broken files\n",
    "#                 return\n",
    "#             if img.height == 0 or img.width == 0:\n",
    "#                 return\n",
    "#             if self.align_long_axis and (\n",
    "#                 (input_size[0] > input_size[1] and img.width > img.height)\n",
    "#                 or (input_size[0] < input_size[1] and img.width < img.height)\n",
    "#             ):\n",
    "#                 img = rotate(img, angle=-90, expand=True)\n",
    "#             img = resize(img, min(input_size))\n",
    "#             img.thumbnail((input_size[1], input_size[0]))\n",
    "#             delta_width = input_size[1] - img.width\n",
    "#             delta_height = input_size[0] - img.height\n",
    "#             if random_padding:\n",
    "#                 pad_width = np.random.randint(low=0, high=delta_width + 1)\n",
    "#                 pad_height = np.random.randint(low=0, high=delta_height + 1)\n",
    "#             else:\n",
    "#                 pad_width = delta_width // 2\n",
    "#                 pad_height = delta_height // 2\n",
    "#             padding = (\n",
    "#                 pad_width,\n",
    "#                 pad_height,\n",
    "#                 delta_width - pad_width,\n",
    "#                 delta_height - pad_height,\n",
    "#             )\n",
    "#             padded_img = ImageOps.expand(img, padding)\n",
    "#             img_list.append(padded_img)\n",
    "\n",
    "#             page_tensor = self.to_tensor(ImageOps.expand(img, padding))\n",
    "#             page_tensor = self.to_tensor(img)\n",
    "#             print(\"Page Tensor: \", page_tensor.shape)\n",
    "\n",
    "#             print(\"page_tensor_shape: \", page_tensor.shape)\n",
    "#             if self.input_tensor is None:\n",
    "#                self.input_tensor = page_tensor\n",
    "#             else:\n",
    "#                 self.input_tensor = torch.cat([self.input_tensor, page_tensor], dim=2)\n",
    "\n",
    "\n",
    "#         # Calculate the padding to be added\n",
    "#         target_shape = (3, self.input_size[0], self.input_size[1])\n",
    "#         original_shape = self.input_tensor.shape\n",
    "#         padding = [0, target_shape[2] - original_shape[2]]\n",
    "\n",
    "#         # Apply padding using torch.nn.functional.pad\n",
    "#         padded_tensor = torch.nn.functional.pad(self.input_tensor, padding)\n",
    "#         img = tensor_to_image(self.input_tensor, '/home/husainmalwat/PDF_2_LaTeX/img_test_horizontal.png')\n",
    "\n",
    "#         return padded_tensor\n",
    "#     \"\"\"\n",
    "\n",
    "    def prepare_input(self, pdf_path:str, random_padding: bool = False):\n",
    "        self.input_tensor = None\n",
    "        if pdf_path is None:\n",
    "            return\n",
    "        input_size= [896,672]\n",
    "        # Convert PDF to images using PyMuPDF\n",
    "        doc = fitz.open(pdf_path)\n",
    "        i=0\n",
    "        for page_number in range(len(doc)):\n",
    "            if i==4:\n",
    "                break\n",
    "            page = doc[page_number]\n",
    "            image = page.get_pixmap()\n",
    "            # Convert Pixmap to PIL Image\n",
    "            img = Image.frombytes(\"RGB\", [image.width, image.height], image.samples)\n",
    "\n",
    "            # crop margins\n",
    "            try:\n",
    "                img = self.crop_margin(img.convert(\"RGB\"))\n",
    "            except OSError:\n",
    "                # might throw an error for broken files\n",
    "                return\n",
    "            if img.height == 0 or img.width == 0:\n",
    "                return\n",
    "            if self.align_long_axis and (\n",
    "                (input_size[0] > input_size[1] and img.width > img.height)\n",
    "                or (input_size[0] < input_size[1] and img.width < img.height)\n",
    "            ):\n",
    "                img = rotate(img, angle=-90, expand=True)\n",
    "            img = resize(img, min(input_size))\n",
    "            img.thumbnail((input_size[1], input_size[0]))\n",
    "            delta_width = input_size[1] - img.width\n",
    "            delta_height = input_size[0] - img.height\n",
    "            if random_padding:\n",
    "                pad_width = np.random.randint(low=0, high=delta_width + 1)\n",
    "                pad_height = np.random.randint(low=0, high=delta_height + 1)\n",
    "            else:\n",
    "                pad_width = delta_width // 2\n",
    "                pad_height = delta_height // 2\n",
    "            padding = (\n",
    "                pad_width,\n",
    "                pad_height,\n",
    "                delta_width - pad_width,\n",
    "                delta_height - pad_height,\n",
    "            )\n",
    "            padded_img = ImageOps.expand(img, padding)\n",
    "\n",
    "            # resize the image (224,224)\n",
    "            # padded_img= padded_img.resize((224,224))\n",
    "            page_tensor = self.to_tensor(padded_img)\n",
    "            # page_tensor = self.to_tensor(ImageOps.expand(img, padding))\n",
    "            print(\"Page Tensor: \", page_tensor.shape)\n",
    "            if self.input_tensor is None:\n",
    "               self.input_tensor = page_tensor\n",
    "            else:\n",
    "                self.input_tensor = torch.cat([self.input_tensor, page_tensor], dim=1)\n",
    "            i+=1\n",
    "\n",
    "        # VERTICAL PADDING added\n",
    "        target_shape = (3, self.input_size[0], self.input_size[1])\n",
    "        padding_needed = (target_shape[1] - self.input_tensor.size(1))\n",
    "        self.input_tensor = torch.nn.functional.pad(self.input_tensor, (0, 0,0, padding_needed))\n",
    "\n",
    "\n",
    "        img = tensor_to_image(self.input_tensor, '/home/husainmalwat/PDF_2_LaTeX/img_test_2.png')\n",
    "\n",
    "        print(\"final Input Tensor: \", )\n",
    "        return self.input_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "def tensor_to_image(input_tensor, save_path):\n",
    "    to_pil = ToPILImage()\n",
    "    img = to_pil(input_tensor)\n",
    "    img.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class pdfDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset_path, split: str = \"train\"):\n",
    "        super().__init__()\n",
    "        self.dataset_path = dataset_path\n",
    "        self.split = split\n",
    "        self.pdf_path = []\n",
    "        self.latex_path = []    \n",
    "\n",
    "        with jsonlines.open(self.dataset_path) as reader:\n",
    "            for line_number, line in enumerate(reader, start=1):\n",
    "                self.pdf_path.append(line['pdf'])\n",
    "                self.latex_path.append(line['latex'])\n",
    "        self.dataset_length = line_number\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pdf_path = self.pdf_path[idx]\n",
    "        latex_path = self.latex_path[idx]\n",
    "\n",
    "        # print(\"pdf: \", pdf_path)\n",
    "        # print(\"latex: \", latex_path)\n",
    "        encoder = enc()\n",
    "        input_tensor = encoder.prepare_input(pdf_path, random_padding=True)\n",
    "\n",
    "        # print(\"input_tensor: \", input_tensor)\n",
    "        print(\"input_tensor.shape from getitem: \", input_tensor.shape)\n",
    "\n",
    "        # with open(latex_path, \"rb\") as f:\n",
    "        #     gnd_truth_data = f.read()\n",
    "        #     try:\n",
    "        #         gnd_truth_data = gnd_truth_data.decode(\"utf-8\")  # Try decoding with UTF-8\n",
    "        #     except:\n",
    "        #         gnd_truth_data = gnd_truth_data.decode(\"latin-1\", errors=\"ignore\")  # Fallback to Latin-1, ignore errors\n",
    "\n",
    "        # # print(\"gnd_truth_data: \", gnd_truth_data)\n",
    "        # tokenizer_out = self.nougat_model.decoder.tokenizer(\n",
    "        #     gnd_truth_data,\n",
    "        #     max_length=self.max_length,\n",
    "        #     padding=\"max_length\",\n",
    "        #     return_token_type_ids=False,\n",
    "        #     truncation=True,\n",
    "        #     return_tensors=\"pt\",\n",
    "        # )\n",
    "        # input_ids = tokenizer_out[\"input_ids\"].squeeze(0)\n",
    "        # attention_mask = tokenizer_out[\"attention_mask\"].squeeze(0)\n",
    "        \"\"\"      \n",
    "        # randomly perturb ground truth tokens\n",
    "        if self.split == \"train\" and self.perturb:\n",
    "            # check if we perturb tokens\n",
    "            unpadded_length = attention_mask.sum()\n",
    "            while random.random() < 0.1:\n",
    "                try:\n",
    "                    pos = random.randint(1, unpadded_length - 2)\n",
    "                    token = random.randint(\n",
    "                        23, len(self.nougat_model.decoder.tokenizer) - 1\n",
    "                    )\n",
    "                    input_ids[pos] = token\n",
    "                except ValueError:\n",
    "                    break\"\"\"\n",
    "        return input_tensor\n",
    "\n",
    "\n",
    "\n",
    "        # Save input_tensor to JSON file\n",
    "        # json_path = '/mnt/NAS/patidarritesh/Pdf_2_LaTeX/Experiments/dataloader.json'\n",
    "        # with open(json_path, \"w\") as file:\n",
    "        #     json.dump(input_tensor.tolist(), file)\n",
    "        \n",
    "\n",
    "\n",
    "        return pdf_path, latex_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pdfDataset(dataset_path=\"/mnt/NAS/patidarritesh/Pdf_2_LaTeX/pdf_2_tex/dataset/root/train.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page Tensor:  torch.Size([3, 896, 672])\n",
      "Page Tensor:  torch.Size([3, 896, 672])\n",
      "Page Tensor:  torch.Size([3, 896, 672])\n",
      "Page Tensor:  torch.Size([3, 896, 672])\n",
      "final Input Tensor: \n",
      "input_tensor.shape from getitem:  torch.Size([3, 4480, 672])\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(dataset):\n",
    "    # print(\"iteration::\",i, sample)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/NAS/patidarritesh/Pdf_2_LaTeX/Experiments/dataloader.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m json_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/mnt/NAS/patidarritesh/Pdf_2_LaTeX/Experiments/dataloader.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjson_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      5\u001b[0m     loaded_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(loaded_data[\u001b[38;5;241m0\u001b[39m])):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/NAS/patidarritesh/Pdf_2_LaTeX/Experiments/dataloader.json'"
     ]
    }
   ],
   "source": [
    "## Padding Details\n",
    "import json\n",
    "json_path = '/mnt/NAS/patidarritesh/Pdf_2_LaTeX/Experiments/dataloader.json'\n",
    "with open(json_path, \"r\") as file:\n",
    "    loaded_data = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "for j in range(len(loaded_data[0])):\n",
    "    count =0\n",
    "    for i in range(len(loaded_data[0][0])):\n",
    "        if loaded_data[0][j][i]!=0 and loaded_data[0][0][i]!=1:\n",
    "            # print(loaded_data[0][0][i])\n",
    "            count+=1\n",
    "    print(\"count: \", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"class pdfDataset(Dataset):\n",
    "\n",
    "    def __init__(self, \n",
    "                dataset_path,\n",
    "                split: str = \"traisn\",\n",
    "                root_name: str = \"pdf\",):\n",
    "        super().__init__()\n",
    "        self.dataset_path = dataset_path\n",
    "        self.split = split\n",
    "        self.root_name = root_name\n",
    "        self.path_to_root = Path(dataset_path).parent\n",
    "        self.root_name = root_name\n",
    "        # self.path_to_root = self.path_to_root.parent\n",
    "\n",
    "        # if not split in self.dataset_path:\n",
    "        #     pti = self.path_to_root / (template % split + \".jsonl\")\n",
    "        #     if pti.exists():\n",
    "        #         self.dataset_path = pti\n",
    "        #     else:\n",
    "        #         raise ValueError(f'Dataset file for split \"{split}\" not found: {pti}')\n",
    "        \n",
    "        self.dataset_file = None  # mulitprocessing\n",
    "        \n",
    "        if self.dataset_file is None:\n",
    "            self.dataset_file = Path(self.dataset_path).open()\n",
    "        line = self.dataset_file.readline()\n",
    "        data: Dict = orjson.loads(line)\n",
    "\n",
    "        pdf_path: Path = self.path_to_root / self.root_name / data.pop(\"pdf\")\n",
    "        latex_path: Path = self.path_to_root / self.root_name / data.pop(\"latex\")\n",
    "\n",
    "        self.dataset = {\"pdf\": pdf_path, \"ground_truth\": latex_path}\n",
    "        self.dataset_length = len(self.dataset)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "\n",
    "        sample = self.dataset[idx]\n",
    "        # print(\"idx:\", idx)\n",
    "        # print(\"sample:\", sample)\n",
    "        return sample\n",
    "        # if sample is None:\n",
    "        #     # if sample is broken choose another randomly\n",
    "        #     return self[random.randint(0, self.dataset_length - 1)]\n",
    "        # if sample is None or sample[\"image\"] is None or prod(sample[\"image\"].size) == 0:\n",
    "        #     input_tensor = None\n",
    "        # else:\n",
    "        #     input_tensor = self.nougat_model.encoder.prepare_input(\n",
    "        #         sample[\"image\"], random_padding=self.split == \"train\"\n",
    "                \n",
    "        #     )\n",
    "\n",
    "        # tokenizer_out = self.nougat_model.decoder.tokenizer(\n",
    "        #     sample[\"ground_truth\"],\n",
    "        #     max_length=self.max_length,\n",
    "        #     padding=\"max_length\",\n",
    "        #     return_token_type_ids=False,\n",
    "        #     truncation=True,\n",
    "        #     return_tensors=\"pt\",\n",
    "        # )\n",
    "        # input_ids = tokenizer_out[\"input_ids\"].squeeze(0)\n",
    "        # attention_mask = tokenizer_out[\"attention_mask\"].squeeze(0)\n",
    "        # # randomly perturb ground truth tokens\n",
    "        # if self.split == \"train\" and self.perturb:\n",
    "        #     # check if we perturb tokens\n",
    "        #     unpadded_length = attention_mask.sum()\n",
    "        #     while random.random() < 0.1:\n",
    "        #         try:\n",
    "        #             pos = random.randint(1, unpadded_length - 2)\n",
    "        #             token = random.randint(\n",
    "        #                 23, len(self.nougat_model.decoder.tokenizer) - 1\n",
    "        #             )\n",
    "        #             input_ids[pos] = token\n",
    "        #         except ValueError:\n",
    "        #             break\n",
    "        # return input_tensor, input_ids, attention_mask\n",
    "    def __iter__(self):\n",
    "        for i in range(self.dataset_length):\n",
    "            yield self[i]\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "def pdf_to_imgs(pdf_path):  \n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    for page_number in range(len(doc)):\n",
    "        page = doc[page_number]\n",
    "        image = page.get_pixmap()\n",
    "        # Convert Pixmap to PIL Image\n",
    "        img = Image.frombytes(\"RGB\", [image.width, image.height], image.samples)\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "\n",
    "\n",
    "def load_latex_code(self, pdf_path):\n",
    "    # Implement this function to extract LaTeX code from the PDF\n",
    "    # You may use a PDF parsing library or any other method\n",
    "    # Return the extracted LaTeX code as a string\n",
    "    latex_code = \"Your LaTeX code here\"\n",
    "    return latex_code\n",
    "\n",
    "def images_to_tensor(self, images):\n",
    "    # Convert a list of images to a PyTorch tensor\n",
    "    tensor_list = [self.transform(image) for image in images]\n",
    "    return torch.stack(tensor_list, dim=0)\n",
    "\n",
    "imgs = pdf_to_imgs(\"/mnt/NAS/patidarritesh/swin_transformer.pdf\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def crop_margin(img: Image.Image) -> Image.Image:\n",
    "        data = np.array(img.convert(\"L\"))\n",
    "        data = data.astype(np.uint8)\n",
    "        max_val = data.max()\n",
    "        min_val = data.min()\n",
    "        if max_val == min_val:\n",
    "            return img\n",
    "        data = (data - min_val) / (max_val - min_val) * 255\n",
    "        gray = 255 * (data < 200).astype(np.uint8)\n",
    "\n",
    "        coords = cv2.findNonZero(gray)  # Find all non-zero points (text)\n",
    "        a, b, w, h = cv2.boundingRect(coords)  # Find minimum spanning bounding box\n",
    "        return img.crop((a, b, w + a, h + b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = imgs[0]\n",
    "\n",
    "image = crop_margin(image.convert(\"RGB\"))\n",
    "\n",
    "# if ((input_size[0] > input_size[1] and img.width > img.height)\n",
    "#                 or (input_size[0] < input_size[1] and img.width < img.height)\n",
    "#             ):\n",
    "#                 img = rotate(img, angle=-90, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(612, 792)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(528, 606)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Height:  606\n",
      "Width:  528\n"
     ]
    }
   ],
   "source": [
    "print(\"Height: \",image.height)\n",
    "print(\"Width: \", image.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.save(\"output.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs[0].save(\"output_1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patidarritesh/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms.functional import resize, rotate\n",
    "image = resize(image, min([896,224]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 257)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257.09090909090907"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=606*224/528\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(self, img: Image.Image):\n",
    "        transform_list = []\n",
    "        \n",
    "        # Add Resize transformation if needed\n",
    "        if self.training:\n",
    "            transform_list.append(transforms.Resize((896, 224)))\n",
    "\n",
    "        # Add ToTensor transformation\n",
    "        transform_list.append(transforms.ToTensor())\n",
    "\n",
    "        composed_transform = transforms.Compose(transform_list)\n",
    "        return composed_transform(img)\n",
    "    \n",
    "    def prepare_input(self, pdf_path:str, random_padding: bool = False):\n",
    "        self.input_tensor = None\n",
    "        if pdf_path is None:\n",
    "            return\n",
    "        input_size= [896,672]\n",
    "        # Convert PDF to images using PyMuPDF\n",
    "        doc = fitz.open(pdf_path)\n",
    "        i=0\n",
    "        for page_number in range(len(doc)):\n",
    "            if i==5:\n",
    "                break\n",
    "            page = doc[page_number]\n",
    "            image = page.get_pixmap()\n",
    "            # Convert Pixmap to PIL Image\n",
    "            img = Image.frombytes(\"RGB\", [image.width, image.height], image.samples)\n",
    "            print(\"raw img: \",img.size)\n",
    "            # crop margins\n",
    "            try:\n",
    "                img = self.crop_margin(img.convert(\"RGB\"))\n",
    "                print(\"img after cropt margin: \",img.size)\n",
    "                print(\"-----------------------------------------------\")\n",
    "            except OSError:\n",
    "                # might throw an error for broken files\n",
    "                return\n",
    "            if img.height == 0 or img.width == 0:\n",
    "                return\n",
    "            if self.align_long_axis and (\n",
    "                (input_size[0] > input_size[1] and img.width > img.height)\n",
    "                or (input_size[0] < input_size[1] and img.width < img.height)\n",
    "            ):\n",
    "                img = rotate(img, angle=-90, expand=True)\n",
    "            img = resize(img, min(self.input_size))\n",
    "            img.thumbnail((input_size[1], input_size[0]))\n",
    "            delta_width = input_size[1] - img.width\n",
    "            delta_height = input_size[0] - img.height\n",
    "            if random_padding:\n",
    "                pad_width = np.random.randint(low=0, high=delta_width + 1)\n",
    "                pad_height = np.random.randint(low=0, high=delta_height + 1)\n",
    "            else:\n",
    "                pad_width = delta_width // 2\n",
    "                pad_height = delta_height // 2\n",
    "            padding = (\n",
    "                pad_width,\n",
    "                pad_height,\n",
    "                delta_width - pad_width,\n",
    "                delta_height - pad_height,\n",
    "            )\n",
    "            \n",
    "            page_tensor = self.to_tensor(ImageOps.expand(img, padding))\n",
    "            print(\"Final transformed page_tensor for single page: \",page_tensor.shape)\n",
    "            if self.input_tensor is None:\n",
    "               self.input_tensor = page_tensor\n",
    "            else:\n",
    "                self.input_tensor = torch.cat([self.input_tensor, page_tensor], dim=2)\n",
    "            i+=1\n",
    "\n",
    "        target_shape = (3, self.input_size[0], self.input_size[1])\n",
    "        original_shape = self.input_tensor.shape\n",
    "        padding = [0, target_shape[2] - original_shape[2]]\n",
    "\n",
    "        # Apply padding using torch.nn.functional.pad\n",
    "        self.input_tensor = torch.nn.functional.pad(self.input_tensor, padding)\n",
    "        print(\"input_tensor  after paddig: \",self.input_tensor.shape)\n",
    "        return self.input_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patidarritesh/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import ImageOps\n",
    "from timm.models.swin_transformer import SwinTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patidarritesh/miniconda3/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "model = SwinTransformer(\n",
    "            # img_size=self.input_size,\n",
    "            # depths=self.encoder_layer,\n",
    "            # window_size=self.window_size,\n",
    "            # patch_size=self.patch_size,\n",
    "            # embed_dim=self.embed_dim,\n",
    "            # num_heads=self.num_heads,\n",
    "            # num_classes=0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "swin_state_dict = timm.create_model(\n",
    "\"swin_base_patch4_window12_384\", pretrained=True\n",
    ").state_dict()\n",
    "new_swin_state_dict =model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_embed.proj.weight\n",
      "patch_embed.proj.weight\n",
      "patch_embed.proj.bias\n",
      "patch_embed.proj.bias\n",
      "patch_embed.norm.weight\n",
      "patch_embed.norm.weight\n",
      "patch_embed.norm.bias\n",
      "patch_embed.norm.bias\n",
      "layers.0.blocks.0.norm1.weight\n",
      "layers.0.blocks.0.norm1.weight\n",
      "layers.0.blocks.0.norm1.bias\n",
      "layers.0.blocks.0.norm1.bias\n",
      "layers.0.blocks.0.attn.relative_position_bias_table\n",
      "layers.0.blocks.0.attn.relative_position_bias_table\n",
      "layers.0.blocks.0.attn.relative_position_index\n",
      "layers.0.blocks.0.attn.relative_position_index\n",
      "layers.0.blocks.0.attn.qkv.weight\n",
      "layers.0.blocks.0.attn.qkv.weight\n",
      "layers.0.blocks.0.attn.qkv.bias\n",
      "layers.0.blocks.0.attn.qkv.bias\n",
      "layers.0.blocks.0.attn.proj.weight\n",
      "layers.0.blocks.0.attn.proj.weight\n",
      "layers.0.blocks.0.attn.proj.bias\n",
      "layers.0.blocks.0.attn.proj.bias\n",
      "layers.0.blocks.0.norm2.weight\n",
      "layers.0.blocks.0.norm2.weight\n",
      "layers.0.blocks.0.norm2.bias\n",
      "layers.0.blocks.0.norm2.bias\n",
      "layers.0.blocks.0.mlp.fc1.weight\n",
      "layers.0.blocks.0.mlp.fc1.weight\n",
      "layers.0.blocks.0.mlp.fc1.bias\n",
      "layers.0.blocks.0.mlp.fc1.bias\n",
      "layers.0.blocks.0.mlp.fc2.weight\n",
      "layers.0.blocks.0.mlp.fc2.weight\n",
      "layers.0.blocks.0.mlp.fc2.bias\n",
      "layers.0.blocks.0.mlp.fc2.bias\n",
      "layers.0.blocks.1.attn_mask\n",
      "layers.0.blocks.1.attn_mask\n",
      "layers.0.blocks.1.norm1.weight\n",
      "layers.0.blocks.1.norm1.weight\n",
      "layers.0.blocks.1.norm1.bias\n",
      "layers.0.blocks.1.norm1.bias\n",
      "layers.0.blocks.1.attn.relative_position_bias_table\n",
      "layers.0.blocks.1.attn.relative_position_bias_table\n",
      "layers.0.blocks.1.attn.relative_position_index\n",
      "layers.0.blocks.1.attn.relative_position_index\n",
      "layers.0.blocks.1.attn.qkv.weight\n",
      "layers.0.blocks.1.attn.qkv.weight\n",
      "layers.0.blocks.1.attn.qkv.bias\n",
      "layers.0.blocks.1.attn.qkv.bias\n",
      "layers.0.blocks.1.attn.proj.weight\n",
      "layers.0.blocks.1.attn.proj.weight\n",
      "layers.0.blocks.1.attn.proj.bias\n",
      "layers.0.blocks.1.attn.proj.bias\n",
      "layers.0.blocks.1.norm2.weight\n",
      "layers.0.blocks.1.norm2.weight\n",
      "layers.0.blocks.1.norm2.bias\n",
      "layers.0.blocks.1.norm2.bias\n",
      "layers.0.blocks.1.mlp.fc1.weight\n",
      "layers.0.blocks.1.mlp.fc1.weight\n",
      "layers.0.blocks.1.mlp.fc1.bias\n",
      "layers.0.blocks.1.mlp.fc1.bias\n",
      "layers.0.blocks.1.mlp.fc2.weight\n",
      "layers.0.blocks.1.mlp.fc2.weight\n",
      "layers.0.blocks.1.mlp.fc2.bias\n",
      "layers.0.blocks.1.mlp.fc2.bias\n",
      "layers.0.downsample.reduction.weight\n",
      "layers.0.downsample.reduction.weight\n",
      "layers.0.downsample.norm.weight\n",
      "layers.0.downsample.norm.weight\n",
      "layers.0.downsample.norm.bias\n",
      "layers.0.downsample.norm.bias\n",
      "layers.1.blocks.0.norm1.weight\n",
      "layers.1.blocks.0.norm1.weight\n",
      "layers.1.blocks.0.norm1.bias\n",
      "layers.1.blocks.0.norm1.bias\n",
      "layers.1.blocks.0.attn.relative_position_bias_table\n",
      "layers.1.blocks.0.attn.relative_position_bias_table\n",
      "layers.1.blocks.0.attn.relative_position_index\n",
      "layers.1.blocks.0.attn.relative_position_index\n",
      "layers.1.blocks.0.attn.qkv.weight\n",
      "layers.1.blocks.0.attn.qkv.weight\n",
      "layers.1.blocks.0.attn.qkv.bias\n",
      "layers.1.blocks.0.attn.qkv.bias\n",
      "layers.1.blocks.0.attn.proj.weight\n",
      "layers.1.blocks.0.attn.proj.weight\n",
      "layers.1.blocks.0.attn.proj.bias\n",
      "layers.1.blocks.0.attn.proj.bias\n",
      "layers.1.blocks.0.norm2.weight\n",
      "layers.1.blocks.0.norm2.weight\n",
      "layers.1.blocks.0.norm2.bias\n",
      "layers.1.blocks.0.norm2.bias\n",
      "layers.1.blocks.0.mlp.fc1.weight\n",
      "layers.1.blocks.0.mlp.fc1.weight\n",
      "layers.1.blocks.0.mlp.fc1.bias\n",
      "layers.1.blocks.0.mlp.fc1.bias\n",
      "layers.1.blocks.0.mlp.fc2.weight\n",
      "layers.1.blocks.0.mlp.fc2.weight\n",
      "layers.1.blocks.0.mlp.fc2.bias\n",
      "layers.1.blocks.0.mlp.fc2.bias\n",
      "layers.1.blocks.1.attn_mask\n",
      "layers.1.blocks.1.attn_mask\n",
      "layers.1.blocks.1.norm1.weight\n",
      "layers.1.blocks.1.norm1.weight\n",
      "layers.1.blocks.1.norm1.bias\n",
      "layers.1.blocks.1.norm1.bias\n",
      "layers.1.blocks.1.attn.relative_position_bias_table\n",
      "layers.1.blocks.1.attn.relative_position_bias_table\n",
      "layers.1.blocks.1.attn.relative_position_index\n",
      "layers.1.blocks.1.attn.relative_position_index\n",
      "layers.1.blocks.1.attn.qkv.weight\n",
      "layers.1.blocks.1.attn.qkv.weight\n",
      "layers.1.blocks.1.attn.qkv.bias\n",
      "layers.1.blocks.1.attn.qkv.bias\n",
      "layers.1.blocks.1.attn.proj.weight\n",
      "layers.1.blocks.1.attn.proj.weight\n",
      "layers.1.blocks.1.attn.proj.bias\n",
      "layers.1.blocks.1.attn.proj.bias\n",
      "layers.1.blocks.1.norm2.weight\n",
      "layers.1.blocks.1.norm2.weight\n",
      "layers.1.blocks.1.norm2.bias\n",
      "layers.1.blocks.1.norm2.bias\n",
      "layers.1.blocks.1.mlp.fc1.weight\n",
      "layers.1.blocks.1.mlp.fc1.weight\n",
      "layers.1.blocks.1.mlp.fc1.bias\n",
      "layers.1.blocks.1.mlp.fc1.bias\n",
      "layers.1.blocks.1.mlp.fc2.weight\n",
      "layers.1.blocks.1.mlp.fc2.weight\n",
      "layers.1.blocks.1.mlp.fc2.bias\n",
      "layers.1.blocks.1.mlp.fc2.bias\n",
      "layers.1.downsample.reduction.weight\n",
      "layers.1.downsample.reduction.weight\n",
      "layers.1.downsample.norm.weight\n",
      "layers.1.downsample.norm.weight\n",
      "layers.1.downsample.norm.bias\n",
      "layers.1.downsample.norm.bias\n",
      "layers.2.blocks.0.norm1.weight\n",
      "layers.2.blocks.0.norm1.weight\n",
      "layers.2.blocks.0.norm1.bias\n",
      "layers.2.blocks.0.norm1.bias\n",
      "layers.2.blocks.0.attn.relative_position_bias_table\n",
      "layers.2.blocks.0.attn.relative_position_bias_table\n",
      "layers.2.blocks.0.attn.relative_position_index\n",
      "layers.2.blocks.0.attn.relative_position_index\n",
      "layers.2.blocks.0.attn.qkv.weight\n",
      "layers.2.blocks.0.attn.qkv.weight\n",
      "layers.2.blocks.0.attn.qkv.bias\n",
      "layers.2.blocks.0.attn.qkv.bias\n",
      "layers.2.blocks.0.attn.proj.weight\n",
      "layers.2.blocks.0.attn.proj.weight\n",
      "layers.2.blocks.0.attn.proj.bias\n",
      "layers.2.blocks.0.attn.proj.bias\n",
      "layers.2.blocks.0.norm2.weight\n",
      "layers.2.blocks.0.norm2.weight\n",
      "layers.2.blocks.0.norm2.bias\n",
      "layers.2.blocks.0.norm2.bias\n",
      "layers.2.blocks.0.mlp.fc1.weight\n",
      "layers.2.blocks.0.mlp.fc1.weight\n",
      "layers.2.blocks.0.mlp.fc1.bias\n",
      "layers.2.blocks.0.mlp.fc1.bias\n",
      "layers.2.blocks.0.mlp.fc2.weight\n",
      "layers.2.blocks.0.mlp.fc2.weight\n",
      "layers.2.blocks.0.mlp.fc2.bias\n",
      "layers.2.blocks.0.mlp.fc2.bias\n",
      "layers.2.blocks.1.attn_mask\n",
      "layers.2.blocks.1.attn_mask\n",
      "layers.2.blocks.1.norm1.weight\n",
      "layers.2.blocks.1.norm1.weight\n",
      "layers.2.blocks.1.norm1.bias\n",
      "layers.2.blocks.1.norm1.bias\n",
      "layers.2.blocks.1.attn.relative_position_bias_table\n",
      "layers.2.blocks.1.attn.relative_position_bias_table\n",
      "layers.2.blocks.1.attn.relative_position_index\n",
      "layers.2.blocks.1.attn.relative_position_index\n",
      "layers.2.blocks.1.attn.qkv.weight\n",
      "layers.2.blocks.1.attn.qkv.weight\n",
      "layers.2.blocks.1.attn.qkv.bias\n",
      "layers.2.blocks.1.attn.qkv.bias\n",
      "layers.2.blocks.1.attn.proj.weight\n",
      "layers.2.blocks.1.attn.proj.weight\n",
      "layers.2.blocks.1.attn.proj.bias\n",
      "layers.2.blocks.1.attn.proj.bias\n",
      "layers.2.blocks.1.norm2.weight\n",
      "layers.2.blocks.1.norm2.weight\n",
      "layers.2.blocks.1.norm2.bias\n",
      "layers.2.blocks.1.norm2.bias\n",
      "layers.2.blocks.1.mlp.fc1.weight\n",
      "layers.2.blocks.1.mlp.fc1.weight\n",
      "layers.2.blocks.1.mlp.fc1.bias\n",
      "layers.2.blocks.1.mlp.fc1.bias\n",
      "layers.2.blocks.1.mlp.fc2.weight\n",
      "layers.2.blocks.1.mlp.fc2.weight\n",
      "layers.2.blocks.1.mlp.fc2.bias\n",
      "layers.2.blocks.1.mlp.fc2.bias\n",
      "layers.2.blocks.2.norm1.weight\n",
      "layers.2.blocks.2.norm1.weight\n",
      "layers.2.blocks.2.norm1.bias\n",
      "layers.2.blocks.2.norm1.bias\n",
      "layers.2.blocks.2.attn.relative_position_bias_table\n",
      "layers.2.blocks.2.attn.relative_position_bias_table\n",
      "layers.2.blocks.2.attn.relative_position_index\n",
      "layers.2.blocks.2.attn.relative_position_index\n",
      "layers.2.blocks.2.attn.qkv.weight\n",
      "layers.2.blocks.2.attn.qkv.weight\n",
      "layers.2.blocks.2.attn.qkv.bias\n",
      "layers.2.blocks.2.attn.qkv.bias\n",
      "layers.2.blocks.2.attn.proj.weight\n",
      "layers.2.blocks.2.attn.proj.weight\n",
      "layers.2.blocks.2.attn.proj.bias\n",
      "layers.2.blocks.2.attn.proj.bias\n",
      "layers.2.blocks.2.norm2.weight\n",
      "layers.2.blocks.2.norm2.weight\n",
      "layers.2.blocks.2.norm2.bias\n",
      "layers.2.blocks.2.norm2.bias\n",
      "layers.2.blocks.2.mlp.fc1.weight\n",
      "layers.2.blocks.2.mlp.fc1.weight\n",
      "layers.2.blocks.2.mlp.fc1.bias\n",
      "layers.2.blocks.2.mlp.fc1.bias\n",
      "layers.2.blocks.2.mlp.fc2.weight\n",
      "layers.2.blocks.2.mlp.fc2.weight\n",
      "layers.2.blocks.2.mlp.fc2.bias\n",
      "layers.2.blocks.2.mlp.fc2.bias\n",
      "layers.2.blocks.3.attn_mask\n",
      "layers.2.blocks.3.attn_mask\n",
      "layers.2.blocks.3.norm1.weight\n",
      "layers.2.blocks.3.norm1.weight\n",
      "layers.2.blocks.3.norm1.bias\n",
      "layers.2.blocks.3.norm1.bias\n",
      "layers.2.blocks.3.attn.relative_position_bias_table\n",
      "layers.2.blocks.3.attn.relative_position_bias_table\n",
      "layers.2.blocks.3.attn.relative_position_index\n",
      "layers.2.blocks.3.attn.relative_position_index\n",
      "layers.2.blocks.3.attn.qkv.weight\n",
      "layers.2.blocks.3.attn.qkv.weight\n",
      "layers.2.blocks.3.attn.qkv.bias\n",
      "layers.2.blocks.3.attn.qkv.bias\n",
      "layers.2.blocks.3.attn.proj.weight\n",
      "layers.2.blocks.3.attn.proj.weight\n",
      "layers.2.blocks.3.attn.proj.bias\n",
      "layers.2.blocks.3.attn.proj.bias\n",
      "layers.2.blocks.3.norm2.weight\n",
      "layers.2.blocks.3.norm2.weight\n",
      "layers.2.blocks.3.norm2.bias\n",
      "layers.2.blocks.3.norm2.bias\n",
      "layers.2.blocks.3.mlp.fc1.weight\n",
      "layers.2.blocks.3.mlp.fc1.weight\n",
      "layers.2.blocks.3.mlp.fc1.bias\n",
      "layers.2.blocks.3.mlp.fc1.bias\n",
      "layers.2.blocks.3.mlp.fc2.weight\n",
      "layers.2.blocks.3.mlp.fc2.weight\n",
      "layers.2.blocks.3.mlp.fc2.bias\n",
      "layers.2.blocks.3.mlp.fc2.bias\n",
      "layers.2.blocks.4.norm1.weight\n",
      "layers.2.blocks.4.norm1.weight\n",
      "layers.2.blocks.4.norm1.bias\n",
      "layers.2.blocks.4.norm1.bias\n",
      "layers.2.blocks.4.attn.relative_position_bias_table\n",
      "layers.2.blocks.4.attn.relative_position_bias_table\n",
      "layers.2.blocks.4.attn.relative_position_index\n",
      "layers.2.blocks.4.attn.relative_position_index\n",
      "layers.2.blocks.4.attn.qkv.weight\n",
      "layers.2.blocks.4.attn.qkv.weight\n",
      "layers.2.blocks.4.attn.qkv.bias\n",
      "layers.2.blocks.4.attn.qkv.bias\n",
      "layers.2.blocks.4.attn.proj.weight\n",
      "layers.2.blocks.4.attn.proj.weight\n",
      "layers.2.blocks.4.attn.proj.bias\n",
      "layers.2.blocks.4.attn.proj.bias\n",
      "layers.2.blocks.4.norm2.weight\n",
      "layers.2.blocks.4.norm2.weight\n",
      "layers.2.blocks.4.norm2.bias\n",
      "layers.2.blocks.4.norm2.bias\n",
      "layers.2.blocks.4.mlp.fc1.weight\n",
      "layers.2.blocks.4.mlp.fc1.weight\n",
      "layers.2.blocks.4.mlp.fc1.bias\n",
      "layers.2.blocks.4.mlp.fc1.bias\n",
      "layers.2.blocks.4.mlp.fc2.weight\n",
      "layers.2.blocks.4.mlp.fc2.weight\n",
      "layers.2.blocks.4.mlp.fc2.bias\n",
      "layers.2.blocks.4.mlp.fc2.bias\n",
      "layers.2.blocks.5.attn_mask\n",
      "layers.2.blocks.5.attn_mask\n",
      "layers.2.blocks.5.norm1.weight\n",
      "layers.2.blocks.5.norm1.weight\n",
      "layers.2.blocks.5.norm1.bias\n",
      "layers.2.blocks.5.norm1.bias\n",
      "layers.2.blocks.5.attn.relative_position_bias_table\n",
      "layers.2.blocks.5.attn.relative_position_bias_table\n",
      "layers.2.blocks.5.attn.relative_position_index\n",
      "layers.2.blocks.5.attn.relative_position_index\n",
      "layers.2.blocks.5.attn.qkv.weight\n",
      "layers.2.blocks.5.attn.qkv.weight\n",
      "layers.2.blocks.5.attn.qkv.bias\n",
      "layers.2.blocks.5.attn.qkv.bias\n",
      "layers.2.blocks.5.attn.proj.weight\n",
      "layers.2.blocks.5.attn.proj.weight\n",
      "layers.2.blocks.5.attn.proj.bias\n",
      "layers.2.blocks.5.attn.proj.bias\n",
      "layers.2.blocks.5.norm2.weight\n",
      "layers.2.blocks.5.norm2.weight\n",
      "layers.2.blocks.5.norm2.bias\n",
      "layers.2.blocks.5.norm2.bias\n",
      "layers.2.blocks.5.mlp.fc1.weight\n",
      "layers.2.blocks.5.mlp.fc1.weight\n",
      "layers.2.blocks.5.mlp.fc1.bias\n",
      "layers.2.blocks.5.mlp.fc1.bias\n",
      "layers.2.blocks.5.mlp.fc2.weight\n",
      "layers.2.blocks.5.mlp.fc2.weight\n",
      "layers.2.blocks.5.mlp.fc2.bias\n",
      "layers.2.blocks.5.mlp.fc2.bias\n",
      "layers.2.downsample.reduction.weight\n",
      "layers.2.downsample.reduction.weight\n",
      "layers.2.downsample.norm.weight\n",
      "layers.2.downsample.norm.weight\n",
      "layers.2.downsample.norm.bias\n",
      "layers.2.downsample.norm.bias\n",
      "layers.3.blocks.0.norm1.weight\n",
      "layers.3.blocks.0.norm1.weight\n",
      "layers.3.blocks.0.norm1.bias\n",
      "layers.3.blocks.0.norm1.bias\n",
      "layers.3.blocks.0.attn.relative_position_bias_table\n",
      "layers.3.blocks.0.attn.relative_position_bias_table\n",
      "layers.3.blocks.0.attn.relative_position_index\n",
      "layers.3.blocks.0.attn.relative_position_index\n",
      "layers.3.blocks.0.attn.qkv.weight\n",
      "layers.3.blocks.0.attn.qkv.weight\n",
      "layers.3.blocks.0.attn.qkv.bias\n",
      "layers.3.blocks.0.attn.qkv.bias\n",
      "layers.3.blocks.0.attn.proj.weight\n",
      "layers.3.blocks.0.attn.proj.weight\n",
      "layers.3.blocks.0.attn.proj.bias\n",
      "layers.3.blocks.0.attn.proj.bias\n",
      "layers.3.blocks.0.norm2.weight\n",
      "layers.3.blocks.0.norm2.weight\n",
      "layers.3.blocks.0.norm2.bias\n",
      "layers.3.blocks.0.norm2.bias\n",
      "layers.3.blocks.0.mlp.fc1.weight\n",
      "layers.3.blocks.0.mlp.fc1.weight\n",
      "layers.3.blocks.0.mlp.fc1.bias\n",
      "layers.3.blocks.0.mlp.fc1.bias\n",
      "layers.3.blocks.0.mlp.fc2.weight\n",
      "layers.3.blocks.0.mlp.fc2.weight\n",
      "layers.3.blocks.0.mlp.fc2.bias\n",
      "layers.3.blocks.0.mlp.fc2.bias\n",
      "layers.3.blocks.1.norm1.weight\n",
      "layers.3.blocks.1.norm1.weight\n",
      "layers.3.blocks.1.norm1.bias\n",
      "layers.3.blocks.1.norm1.bias\n",
      "layers.3.blocks.1.attn.relative_position_bias_table\n",
      "layers.3.blocks.1.attn.relative_position_bias_table\n",
      "layers.3.blocks.1.attn.relative_position_index\n",
      "layers.3.blocks.1.attn.relative_position_index\n",
      "layers.3.blocks.1.attn.qkv.weight\n",
      "layers.3.blocks.1.attn.qkv.weight\n",
      "layers.3.blocks.1.attn.qkv.bias\n",
      "layers.3.blocks.1.attn.qkv.bias\n",
      "layers.3.blocks.1.attn.proj.weight\n",
      "layers.3.blocks.1.attn.proj.weight\n",
      "layers.3.blocks.1.attn.proj.bias\n",
      "layers.3.blocks.1.attn.proj.bias\n",
      "layers.3.blocks.1.norm2.weight\n",
      "layers.3.blocks.1.norm2.weight\n",
      "layers.3.blocks.1.norm2.bias\n",
      "layers.3.blocks.1.norm2.bias\n",
      "layers.3.blocks.1.mlp.fc1.weight\n",
      "layers.3.blocks.1.mlp.fc1.weight\n",
      "layers.3.blocks.1.mlp.fc1.bias\n",
      "layers.3.blocks.1.mlp.fc1.bias\n",
      "layers.3.blocks.1.mlp.fc2.weight\n",
      "layers.3.blocks.1.mlp.fc2.weight\n",
      "layers.3.blocks.1.mlp.fc2.bias\n",
      "layers.3.blocks.1.mlp.fc2.bias\n",
      "norm.weight\n",
      "norm.weight\n",
      "norm.bias\n",
      "norm.bias\n",
      "head.weight\n",
      "head.weight\n",
      "head.bias\n",
      "head.bias\n"
     ]
    }
   ],
   "source": [
    "# print(\"new_swin_state_dict: \",new_swin_state_dict.keys())\n",
    "lst = new_swin_state_dict.keys()\n",
    "\n",
    "for i in lst:\n",
    "\n",
    "    print((i))\n",
    "    # print(\"Window Size: \", model.layers[0].blocks[0].attn.window_size[0])\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=swin_state_dict[\"layers.0.blocks.0.attn.relative_position_bias_table\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([529, 4])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_bias = x.unsqueeze(0)[0]\n",
    "pos_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "window_size = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_len = int(math.sqrt(len(pos_bias)))\n",
    "new_len = int(2 * window_size - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old_len:  23\n",
      "new_len:  13\n"
     ]
    }
   ],
   "source": [
    "print(\"old_len: \",old_len)\n",
    "print(\"new_len: \",new_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "  pos_bias = pos_bias.reshape(1, old_len, old_len, -1).permute(\n",
    "                        0, 3, 1, 2\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 23, 23])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "  pos_bias = F.interpolate(\n",
    "                        pos_bias,\n",
    "                        size=(new_len, new_len),\n",
    "                        mode=\"bicubic\",\n",
    "                        align_corners=False,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 13, 13])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "   new_swin_state_dict[x] = (\n",
    "                        pos_bias.permute(0, 2, 3, 1)\n",
    "                        .reshape(1, new_len**2, -1)\n",
    "                        .squeeze(0)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([169, 4])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " new_swin_state_dict[x].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.swin_transformer_v2 import SwinTransformerV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelV2 = SwinTransformerV2(\n",
    "            # img_size=self.input_size,\n",
    "            # depths=self.encoder_layer,\n",
    "            # window_size=self.window_size,\n",
    "            # patch_size=self.patch_size,\n",
    "            # embed_dim=self.embed_dim,\n",
    "            # num_heads=self.num_heads,\n",
    "            # num_classes=0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "swin_state_dictV2 = timm.create_model(\n",
    "\"swinv2_base_window12to24_192to384\", pretrained=True\n",
    ").state_dict()\n",
    "new_swin_state_dictV2 =modelV2.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformerV2Block(\n",
      "  (attn): WindowAttention(\n",
      "    (cpb_mlp): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=512, out_features=3, bias=False)\n",
      "    )\n",
      "    (qkv): Linear(in_features=96, out_features=288, bias=False)\n",
      "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "    (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "    (softmax): Softmax(dim=-1)\n",
      "  )\n",
      "  (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop_path1): Identity()\n",
      "  (mlp): Mlp(\n",
      "    (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
      "    (act): GELU(approximate='none')\n",
      "    (drop1): Dropout(p=0.0, inplace=False)\n",
      "    (norm): Identity()\n",
      "    (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
      "    (drop2): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "  (drop_path2): Identity()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(modelV2.layers[0].blocks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_embed.proj.weight\n",
      "patch_embed.proj.bias\n",
      "patch_embed.norm.weight\n",
      "patch_embed.norm.bias\n",
      "layers.0.blocks.0.attn.logit_scale\n",
      "layers.0.blocks.0.attn.q_bias\n",
      "layers.0.blocks.0.attn.v_bias\n",
      "layers.0.blocks.0.attn.cpb_mlp.0.weight\n",
      "layers.0.blocks.0.attn.cpb_mlp.0.bias\n",
      "layers.0.blocks.0.attn.cpb_mlp.2.weight\n",
      "layers.0.blocks.0.attn.qkv.weight\n",
      "layers.0.blocks.0.attn.proj.weight\n",
      "layers.0.blocks.0.attn.proj.bias\n",
      "layers.0.blocks.0.norm1.weight\n",
      "layers.0.blocks.0.norm1.bias\n",
      "layers.0.blocks.0.mlp.fc1.weight\n",
      "layers.0.blocks.0.mlp.fc1.bias\n",
      "layers.0.blocks.0.mlp.fc2.weight\n",
      "layers.0.blocks.0.mlp.fc2.bias\n",
      "layers.0.blocks.0.norm2.weight\n",
      "layers.0.blocks.0.norm2.bias\n",
      "layers.0.blocks.1.attn.logit_scale\n",
      "layers.0.blocks.1.attn.q_bias\n",
      "layers.0.blocks.1.attn.v_bias\n",
      "layers.0.blocks.1.attn.cpb_mlp.0.weight\n",
      "layers.0.blocks.1.attn.cpb_mlp.0.bias\n",
      "layers.0.blocks.1.attn.cpb_mlp.2.weight\n",
      "layers.0.blocks.1.attn.qkv.weight\n",
      "layers.0.blocks.1.attn.proj.weight\n",
      "layers.0.blocks.1.attn.proj.bias\n",
      "layers.0.blocks.1.norm1.weight\n",
      "layers.0.blocks.1.norm1.bias\n",
      "layers.0.blocks.1.mlp.fc1.weight\n",
      "layers.0.blocks.1.mlp.fc1.bias\n",
      "layers.0.blocks.1.mlp.fc2.weight\n",
      "layers.0.blocks.1.mlp.fc2.bias\n",
      "layers.0.blocks.1.norm2.weight\n",
      "layers.0.blocks.1.norm2.bias\n",
      "layers.1.downsample.reduction.weight\n",
      "layers.1.downsample.norm.weight\n",
      "layers.1.downsample.norm.bias\n",
      "layers.1.blocks.0.attn.logit_scale\n",
      "layers.1.blocks.0.attn.q_bias\n",
      "layers.1.blocks.0.attn.v_bias\n",
      "layers.1.blocks.0.attn.cpb_mlp.0.weight\n",
      "layers.1.blocks.0.attn.cpb_mlp.0.bias\n",
      "layers.1.blocks.0.attn.cpb_mlp.2.weight\n",
      "layers.1.blocks.0.attn.qkv.weight\n",
      "layers.1.blocks.0.attn.proj.weight\n",
      "layers.1.blocks.0.attn.proj.bias\n",
      "layers.1.blocks.0.norm1.weight\n",
      "layers.1.blocks.0.norm1.bias\n",
      "layers.1.blocks.0.mlp.fc1.weight\n",
      "layers.1.blocks.0.mlp.fc1.bias\n",
      "layers.1.blocks.0.mlp.fc2.weight\n",
      "layers.1.blocks.0.mlp.fc2.bias\n",
      "layers.1.blocks.0.norm2.weight\n",
      "layers.1.blocks.0.norm2.bias\n",
      "layers.1.blocks.1.attn.logit_scale\n",
      "layers.1.blocks.1.attn.q_bias\n",
      "layers.1.blocks.1.attn.v_bias\n",
      "layers.1.blocks.1.attn.cpb_mlp.0.weight\n",
      "layers.1.blocks.1.attn.cpb_mlp.0.bias\n",
      "layers.1.blocks.1.attn.cpb_mlp.2.weight\n",
      "layers.1.blocks.1.attn.qkv.weight\n",
      "layers.1.blocks.1.attn.proj.weight\n",
      "layers.1.blocks.1.attn.proj.bias\n",
      "layers.1.blocks.1.norm1.weight\n",
      "layers.1.blocks.1.norm1.bias\n",
      "layers.1.blocks.1.mlp.fc1.weight\n",
      "layers.1.blocks.1.mlp.fc1.bias\n",
      "layers.1.blocks.1.mlp.fc2.weight\n",
      "layers.1.blocks.1.mlp.fc2.bias\n",
      "layers.1.blocks.1.norm2.weight\n",
      "layers.1.blocks.1.norm2.bias\n",
      "layers.2.downsample.reduction.weight\n",
      "layers.2.downsample.norm.weight\n",
      "layers.2.downsample.norm.bias\n",
      "layers.2.blocks.0.attn.logit_scale\n",
      "layers.2.blocks.0.attn.q_bias\n",
      "layers.2.blocks.0.attn.v_bias\n",
      "layers.2.blocks.0.attn.cpb_mlp.0.weight\n",
      "layers.2.blocks.0.attn.cpb_mlp.0.bias\n",
      "layers.2.blocks.0.attn.cpb_mlp.2.weight\n",
      "layers.2.blocks.0.attn.qkv.weight\n",
      "layers.2.blocks.0.attn.proj.weight\n",
      "layers.2.blocks.0.attn.proj.bias\n",
      "layers.2.blocks.0.norm1.weight\n",
      "layers.2.blocks.0.norm1.bias\n",
      "layers.2.blocks.0.mlp.fc1.weight\n",
      "layers.2.blocks.0.mlp.fc1.bias\n",
      "layers.2.blocks.0.mlp.fc2.weight\n",
      "layers.2.blocks.0.mlp.fc2.bias\n",
      "layers.2.blocks.0.norm2.weight\n",
      "layers.2.blocks.0.norm2.bias\n",
      "layers.2.blocks.1.attn.logit_scale\n",
      "layers.2.blocks.1.attn.q_bias\n",
      "layers.2.blocks.1.attn.v_bias\n",
      "layers.2.blocks.1.attn.cpb_mlp.0.weight\n",
      "layers.2.blocks.1.attn.cpb_mlp.0.bias\n",
      "layers.2.blocks.1.attn.cpb_mlp.2.weight\n",
      "layers.2.blocks.1.attn.qkv.weight\n",
      "layers.2.blocks.1.attn.proj.weight\n",
      "layers.2.blocks.1.attn.proj.bias\n",
      "layers.2.blocks.1.norm1.weight\n",
      "layers.2.blocks.1.norm1.bias\n",
      "layers.2.blocks.1.mlp.fc1.weight\n",
      "layers.2.blocks.1.mlp.fc1.bias\n",
      "layers.2.blocks.1.mlp.fc2.weight\n",
      "layers.2.blocks.1.mlp.fc2.bias\n",
      "layers.2.blocks.1.norm2.weight\n",
      "layers.2.blocks.1.norm2.bias\n",
      "layers.2.blocks.2.attn.logit_scale\n",
      "layers.2.blocks.2.attn.q_bias\n",
      "layers.2.blocks.2.attn.v_bias\n",
      "layers.2.blocks.2.attn.cpb_mlp.0.weight\n",
      "layers.2.blocks.2.attn.cpb_mlp.0.bias\n",
      "layers.2.blocks.2.attn.cpb_mlp.2.weight\n",
      "layers.2.blocks.2.attn.qkv.weight\n",
      "layers.2.blocks.2.attn.proj.weight\n",
      "layers.2.blocks.2.attn.proj.bias\n",
      "layers.2.blocks.2.norm1.weight\n",
      "layers.2.blocks.2.norm1.bias\n",
      "layers.2.blocks.2.mlp.fc1.weight\n",
      "layers.2.blocks.2.mlp.fc1.bias\n",
      "layers.2.blocks.2.mlp.fc2.weight\n",
      "layers.2.blocks.2.mlp.fc2.bias\n",
      "layers.2.blocks.2.norm2.weight\n",
      "layers.2.blocks.2.norm2.bias\n",
      "layers.2.blocks.3.attn.logit_scale\n",
      "layers.2.blocks.3.attn.q_bias\n",
      "layers.2.blocks.3.attn.v_bias\n",
      "layers.2.blocks.3.attn.cpb_mlp.0.weight\n",
      "layers.2.blocks.3.attn.cpb_mlp.0.bias\n",
      "layers.2.blocks.3.attn.cpb_mlp.2.weight\n",
      "layers.2.blocks.3.attn.qkv.weight\n",
      "layers.2.blocks.3.attn.proj.weight\n",
      "layers.2.blocks.3.attn.proj.bias\n",
      "layers.2.blocks.3.norm1.weight\n",
      "layers.2.blocks.3.norm1.bias\n",
      "layers.2.blocks.3.mlp.fc1.weight\n",
      "layers.2.blocks.3.mlp.fc1.bias\n",
      "layers.2.blocks.3.mlp.fc2.weight\n",
      "layers.2.blocks.3.mlp.fc2.bias\n",
      "layers.2.blocks.3.norm2.weight\n",
      "layers.2.blocks.3.norm2.bias\n",
      "layers.2.blocks.4.attn.logit_scale\n",
      "layers.2.blocks.4.attn.q_bias\n",
      "layers.2.blocks.4.attn.v_bias\n",
      "layers.2.blocks.4.attn.cpb_mlp.0.weight\n",
      "layers.2.blocks.4.attn.cpb_mlp.0.bias\n",
      "layers.2.blocks.4.attn.cpb_mlp.2.weight\n",
      "layers.2.blocks.4.attn.qkv.weight\n",
      "layers.2.blocks.4.attn.proj.weight\n",
      "layers.2.blocks.4.attn.proj.bias\n",
      "layers.2.blocks.4.norm1.weight\n",
      "layers.2.blocks.4.norm1.bias\n",
      "layers.2.blocks.4.mlp.fc1.weight\n",
      "layers.2.blocks.4.mlp.fc1.bias\n",
      "layers.2.blocks.4.mlp.fc2.weight\n",
      "layers.2.blocks.4.mlp.fc2.bias\n",
      "layers.2.blocks.4.norm2.weight\n",
      "layers.2.blocks.4.norm2.bias\n",
      "layers.2.blocks.5.attn.logit_scale\n",
      "layers.2.blocks.5.attn.q_bias\n",
      "layers.2.blocks.5.attn.v_bias\n",
      "layers.2.blocks.5.attn.cpb_mlp.0.weight\n",
      "layers.2.blocks.5.attn.cpb_mlp.0.bias\n",
      "layers.2.blocks.5.attn.cpb_mlp.2.weight\n",
      "layers.2.blocks.5.attn.qkv.weight\n",
      "layers.2.blocks.5.attn.proj.weight\n",
      "layers.2.blocks.5.attn.proj.bias\n",
      "layers.2.blocks.5.norm1.weight\n",
      "layers.2.blocks.5.norm1.bias\n",
      "layers.2.blocks.5.mlp.fc1.weight\n",
      "layers.2.blocks.5.mlp.fc1.bias\n",
      "layers.2.blocks.5.mlp.fc2.weight\n",
      "layers.2.blocks.5.mlp.fc2.bias\n",
      "layers.2.blocks.5.norm2.weight\n",
      "layers.2.blocks.5.norm2.bias\n",
      "layers.3.downsample.reduction.weight\n",
      "layers.3.downsample.norm.weight\n",
      "layers.3.downsample.norm.bias\n",
      "layers.3.blocks.0.attn.logit_scale\n",
      "layers.3.blocks.0.attn.q_bias\n",
      "layers.3.blocks.0.attn.v_bias\n",
      "layers.3.blocks.0.attn.cpb_mlp.0.weight\n",
      "layers.3.blocks.0.attn.cpb_mlp.0.bias\n",
      "layers.3.blocks.0.attn.cpb_mlp.2.weight\n",
      "layers.3.blocks.0.attn.qkv.weight\n",
      "layers.3.blocks.0.attn.proj.weight\n",
      "layers.3.blocks.0.attn.proj.bias\n",
      "layers.3.blocks.0.norm1.weight\n",
      "layers.3.blocks.0.norm1.bias\n",
      "layers.3.blocks.0.mlp.fc1.weight\n",
      "layers.3.blocks.0.mlp.fc1.bias\n",
      "layers.3.blocks.0.mlp.fc2.weight\n",
      "layers.3.blocks.0.mlp.fc2.bias\n",
      "layers.3.blocks.0.norm2.weight\n",
      "layers.3.blocks.0.norm2.bias\n",
      "layers.3.blocks.1.attn.logit_scale\n",
      "layers.3.blocks.1.attn.q_bias\n",
      "layers.3.blocks.1.attn.v_bias\n",
      "layers.3.blocks.1.attn.cpb_mlp.0.weight\n",
      "layers.3.blocks.1.attn.cpb_mlp.0.bias\n",
      "layers.3.blocks.1.attn.cpb_mlp.2.weight\n",
      "layers.3.blocks.1.attn.qkv.weight\n",
      "layers.3.blocks.1.attn.proj.weight\n",
      "layers.3.blocks.1.attn.proj.bias\n",
      "layers.3.blocks.1.norm1.weight\n",
      "layers.3.blocks.1.norm1.bias\n",
      "layers.3.blocks.1.mlp.fc1.weight\n",
      "layers.3.blocks.1.mlp.fc1.bias\n",
      "layers.3.blocks.1.mlp.fc2.weight\n",
      "layers.3.blocks.1.mlp.fc2.bias\n",
      "layers.3.blocks.1.norm2.weight\n",
      "layers.3.blocks.1.norm2.bias\n",
      "norm.weight\n",
      "norm.bias\n",
      "head.fc.weight\n",
      "head.fc.bias\n"
     ]
    }
   ],
   "source": [
    "lst = new_swin_state_dictV2.keys()\n",
    "j=0\n",
    "for i in lst:\n",
    "    # if i.endswith(\"relative_position_bias_table\") : # and modelV2.layers[0].blocks[0].attn.window_size[0] != 12\n",
    "        print(i)\n",
    "        # print(\"Window Size: \", modelV2.layers[0].blocks[0].attn.window_size[0])\n",
    "        j+=1\n",
    "    # print(list(i.split('.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swin_state_dictV2: odict_keys(['patch_embed.proj.weight', 'patch_embed.proj.bias', 'patch_embed.norm.weight', 'patch_embed.norm.bias', 'layers.0.blocks.0.attn.logit_scale', 'layers.0.blocks.0.attn.q_bias', 'layers.0.blocks.0.attn.v_bias', 'layers.0.blocks.0.attn.cpb_mlp.0.weight', 'layers.0.blocks.0.attn.cpb_mlp.0.bias', 'layers.0.blocks.0.attn.cpb_mlp.2.weight', 'layers.0.blocks.0.attn.qkv.weight', 'layers.0.blocks.0.attn.proj.weight', 'layers.0.blocks.0.attn.proj.bias', 'layers.0.blocks.0.norm1.weight', 'layers.0.blocks.0.norm1.bias', 'layers.0.blocks.0.mlp.fc1.weight', 'layers.0.blocks.0.mlp.fc1.bias', 'layers.0.blocks.0.mlp.fc2.weight', 'layers.0.blocks.0.mlp.fc2.bias', 'layers.0.blocks.0.norm2.weight', 'layers.0.blocks.0.norm2.bias', 'layers.0.blocks.1.attn.logit_scale', 'layers.0.blocks.1.attn.q_bias', 'layers.0.blocks.1.attn.v_bias', 'layers.0.blocks.1.attn.cpb_mlp.0.weight', 'layers.0.blocks.1.attn.cpb_mlp.0.bias', 'layers.0.blocks.1.attn.cpb_mlp.2.weight', 'layers.0.blocks.1.attn.qkv.weight', 'layers.0.blocks.1.attn.proj.weight', 'layers.0.blocks.1.attn.proj.bias', 'layers.0.blocks.1.norm1.weight', 'layers.0.blocks.1.norm1.bias', 'layers.0.blocks.1.mlp.fc1.weight', 'layers.0.blocks.1.mlp.fc1.bias', 'layers.0.blocks.1.mlp.fc2.weight', 'layers.0.blocks.1.mlp.fc2.bias', 'layers.0.blocks.1.norm2.weight', 'layers.0.blocks.1.norm2.bias', 'layers.1.downsample.reduction.weight', 'layers.1.downsample.norm.weight', 'layers.1.downsample.norm.bias', 'layers.1.blocks.0.attn.logit_scale', 'layers.1.blocks.0.attn.q_bias', 'layers.1.blocks.0.attn.v_bias', 'layers.1.blocks.0.attn.cpb_mlp.0.weight', 'layers.1.blocks.0.attn.cpb_mlp.0.bias', 'layers.1.blocks.0.attn.cpb_mlp.2.weight', 'layers.1.blocks.0.attn.qkv.weight', 'layers.1.blocks.0.attn.proj.weight', 'layers.1.blocks.0.attn.proj.bias', 'layers.1.blocks.0.norm1.weight', 'layers.1.blocks.0.norm1.bias', 'layers.1.blocks.0.mlp.fc1.weight', 'layers.1.blocks.0.mlp.fc1.bias', 'layers.1.blocks.0.mlp.fc2.weight', 'layers.1.blocks.0.mlp.fc2.bias', 'layers.1.blocks.0.norm2.weight', 'layers.1.blocks.0.norm2.bias', 'layers.1.blocks.1.attn.logit_scale', 'layers.1.blocks.1.attn.q_bias', 'layers.1.blocks.1.attn.v_bias', 'layers.1.blocks.1.attn.cpb_mlp.0.weight', 'layers.1.blocks.1.attn.cpb_mlp.0.bias', 'layers.1.blocks.1.attn.cpb_mlp.2.weight', 'layers.1.blocks.1.attn.qkv.weight', 'layers.1.blocks.1.attn.proj.weight', 'layers.1.blocks.1.attn.proj.bias', 'layers.1.blocks.1.norm1.weight', 'layers.1.blocks.1.norm1.bias', 'layers.1.blocks.1.mlp.fc1.weight', 'layers.1.blocks.1.mlp.fc1.bias', 'layers.1.blocks.1.mlp.fc2.weight', 'layers.1.blocks.1.mlp.fc2.bias', 'layers.1.blocks.1.norm2.weight', 'layers.1.blocks.1.norm2.bias', 'layers.2.downsample.reduction.weight', 'layers.2.downsample.norm.weight', 'layers.2.downsample.norm.bias', 'layers.2.blocks.0.attn.logit_scale', 'layers.2.blocks.0.attn.q_bias', 'layers.2.blocks.0.attn.v_bias', 'layers.2.blocks.0.attn.cpb_mlp.0.weight', 'layers.2.blocks.0.attn.cpb_mlp.0.bias', 'layers.2.blocks.0.attn.cpb_mlp.2.weight', 'layers.2.blocks.0.attn.qkv.weight', 'layers.2.blocks.0.attn.proj.weight', 'layers.2.blocks.0.attn.proj.bias', 'layers.2.blocks.0.norm1.weight', 'layers.2.blocks.0.norm1.bias', 'layers.2.blocks.0.mlp.fc1.weight', 'layers.2.blocks.0.mlp.fc1.bias', 'layers.2.blocks.0.mlp.fc2.weight', 'layers.2.blocks.0.mlp.fc2.bias', 'layers.2.blocks.0.norm2.weight', 'layers.2.blocks.0.norm2.bias', 'layers.2.blocks.1.attn.logit_scale', 'layers.2.blocks.1.attn.q_bias', 'layers.2.blocks.1.attn.v_bias', 'layers.2.blocks.1.attn.cpb_mlp.0.weight', 'layers.2.blocks.1.attn.cpb_mlp.0.bias', 'layers.2.blocks.1.attn.cpb_mlp.2.weight', 'layers.2.blocks.1.attn.qkv.weight', 'layers.2.blocks.1.attn.proj.weight', 'layers.2.blocks.1.attn.proj.bias', 'layers.2.blocks.1.norm1.weight', 'layers.2.blocks.1.norm1.bias', 'layers.2.blocks.1.mlp.fc1.weight', 'layers.2.blocks.1.mlp.fc1.bias', 'layers.2.blocks.1.mlp.fc2.weight', 'layers.2.blocks.1.mlp.fc2.bias', 'layers.2.blocks.1.norm2.weight', 'layers.2.blocks.1.norm2.bias', 'layers.2.blocks.2.attn.logit_scale', 'layers.2.blocks.2.attn.q_bias', 'layers.2.blocks.2.attn.v_bias', 'layers.2.blocks.2.attn.cpb_mlp.0.weight', 'layers.2.blocks.2.attn.cpb_mlp.0.bias', 'layers.2.blocks.2.attn.cpb_mlp.2.weight', 'layers.2.blocks.2.attn.qkv.weight', 'layers.2.blocks.2.attn.proj.weight', 'layers.2.blocks.2.attn.proj.bias', 'layers.2.blocks.2.norm1.weight', 'layers.2.blocks.2.norm1.bias', 'layers.2.blocks.2.mlp.fc1.weight', 'layers.2.blocks.2.mlp.fc1.bias', 'layers.2.blocks.2.mlp.fc2.weight', 'layers.2.blocks.2.mlp.fc2.bias', 'layers.2.blocks.2.norm2.weight', 'layers.2.blocks.2.norm2.bias', 'layers.2.blocks.3.attn.logit_scale', 'layers.2.blocks.3.attn.q_bias', 'layers.2.blocks.3.attn.v_bias', 'layers.2.blocks.3.attn.cpb_mlp.0.weight', 'layers.2.blocks.3.attn.cpb_mlp.0.bias', 'layers.2.blocks.3.attn.cpb_mlp.2.weight', 'layers.2.blocks.3.attn.qkv.weight', 'layers.2.blocks.3.attn.proj.weight', 'layers.2.blocks.3.attn.proj.bias', 'layers.2.blocks.3.norm1.weight', 'layers.2.blocks.3.norm1.bias', 'layers.2.blocks.3.mlp.fc1.weight', 'layers.2.blocks.3.mlp.fc1.bias', 'layers.2.blocks.3.mlp.fc2.weight', 'layers.2.blocks.3.mlp.fc2.bias', 'layers.2.blocks.3.norm2.weight', 'layers.2.blocks.3.norm2.bias', 'layers.2.blocks.4.attn.logit_scale', 'layers.2.blocks.4.attn.q_bias', 'layers.2.blocks.4.attn.v_bias', 'layers.2.blocks.4.attn.cpb_mlp.0.weight', 'layers.2.blocks.4.attn.cpb_mlp.0.bias', 'layers.2.blocks.4.attn.cpb_mlp.2.weight', 'layers.2.blocks.4.attn.qkv.weight', 'layers.2.blocks.4.attn.proj.weight', 'layers.2.blocks.4.attn.proj.bias', 'layers.2.blocks.4.norm1.weight', 'layers.2.blocks.4.norm1.bias', 'layers.2.blocks.4.mlp.fc1.weight', 'layers.2.blocks.4.mlp.fc1.bias', 'layers.2.blocks.4.mlp.fc2.weight', 'layers.2.blocks.4.mlp.fc2.bias', 'layers.2.blocks.4.norm2.weight', 'layers.2.blocks.4.norm2.bias', 'layers.2.blocks.5.attn.logit_scale', 'layers.2.blocks.5.attn.q_bias', 'layers.2.blocks.5.attn.v_bias', 'layers.2.blocks.5.attn.cpb_mlp.0.weight', 'layers.2.blocks.5.attn.cpb_mlp.0.bias', 'layers.2.blocks.5.attn.cpb_mlp.2.weight', 'layers.2.blocks.5.attn.qkv.weight', 'layers.2.blocks.5.attn.proj.weight', 'layers.2.blocks.5.attn.proj.bias', 'layers.2.blocks.5.norm1.weight', 'layers.2.blocks.5.norm1.bias', 'layers.2.blocks.5.mlp.fc1.weight', 'layers.2.blocks.5.mlp.fc1.bias', 'layers.2.blocks.5.mlp.fc2.weight', 'layers.2.blocks.5.mlp.fc2.bias', 'layers.2.blocks.5.norm2.weight', 'layers.2.blocks.5.norm2.bias', 'layers.2.blocks.6.attn.logit_scale', 'layers.2.blocks.6.attn.q_bias', 'layers.2.blocks.6.attn.v_bias', 'layers.2.blocks.6.attn.cpb_mlp.0.weight', 'layers.2.blocks.6.attn.cpb_mlp.0.bias', 'layers.2.blocks.6.attn.cpb_mlp.2.weight', 'layers.2.blocks.6.attn.qkv.weight', 'layers.2.blocks.6.attn.proj.weight', 'layers.2.blocks.6.attn.proj.bias', 'layers.2.blocks.6.norm1.weight', 'layers.2.blocks.6.norm1.bias', 'layers.2.blocks.6.mlp.fc1.weight', 'layers.2.blocks.6.mlp.fc1.bias', 'layers.2.blocks.6.mlp.fc2.weight', 'layers.2.blocks.6.mlp.fc2.bias', 'layers.2.blocks.6.norm2.weight', 'layers.2.blocks.6.norm2.bias', 'layers.2.blocks.7.attn.logit_scale', 'layers.2.blocks.7.attn.q_bias', 'layers.2.blocks.7.attn.v_bias', 'layers.2.blocks.7.attn.cpb_mlp.0.weight', 'layers.2.blocks.7.attn.cpb_mlp.0.bias', 'layers.2.blocks.7.attn.cpb_mlp.2.weight', 'layers.2.blocks.7.attn.qkv.weight', 'layers.2.blocks.7.attn.proj.weight', 'layers.2.blocks.7.attn.proj.bias', 'layers.2.blocks.7.norm1.weight', 'layers.2.blocks.7.norm1.bias', 'layers.2.blocks.7.mlp.fc1.weight', 'layers.2.blocks.7.mlp.fc1.bias', 'layers.2.blocks.7.mlp.fc2.weight', 'layers.2.blocks.7.mlp.fc2.bias', 'layers.2.blocks.7.norm2.weight', 'layers.2.blocks.7.norm2.bias', 'layers.2.blocks.8.attn.logit_scale', 'layers.2.blocks.8.attn.q_bias', 'layers.2.blocks.8.attn.v_bias', 'layers.2.blocks.8.attn.cpb_mlp.0.weight', 'layers.2.blocks.8.attn.cpb_mlp.0.bias', 'layers.2.blocks.8.attn.cpb_mlp.2.weight', 'layers.2.blocks.8.attn.qkv.weight', 'layers.2.blocks.8.attn.proj.weight', 'layers.2.blocks.8.attn.proj.bias', 'layers.2.blocks.8.norm1.weight', 'layers.2.blocks.8.norm1.bias', 'layers.2.blocks.8.mlp.fc1.weight', 'layers.2.blocks.8.mlp.fc1.bias', 'layers.2.blocks.8.mlp.fc2.weight', 'layers.2.blocks.8.mlp.fc2.bias', 'layers.2.blocks.8.norm2.weight', 'layers.2.blocks.8.norm2.bias', 'layers.2.blocks.9.attn.logit_scale', 'layers.2.blocks.9.attn.q_bias', 'layers.2.blocks.9.attn.v_bias', 'layers.2.blocks.9.attn.cpb_mlp.0.weight', 'layers.2.blocks.9.attn.cpb_mlp.0.bias', 'layers.2.blocks.9.attn.cpb_mlp.2.weight', 'layers.2.blocks.9.attn.qkv.weight', 'layers.2.blocks.9.attn.proj.weight', 'layers.2.blocks.9.attn.proj.bias', 'layers.2.blocks.9.norm1.weight', 'layers.2.blocks.9.norm1.bias', 'layers.2.blocks.9.mlp.fc1.weight', 'layers.2.blocks.9.mlp.fc1.bias', 'layers.2.blocks.9.mlp.fc2.weight', 'layers.2.blocks.9.mlp.fc2.bias', 'layers.2.blocks.9.norm2.weight', 'layers.2.blocks.9.norm2.bias', 'layers.2.blocks.10.attn.logit_scale', 'layers.2.blocks.10.attn.q_bias', 'layers.2.blocks.10.attn.v_bias', 'layers.2.blocks.10.attn.cpb_mlp.0.weight', 'layers.2.blocks.10.attn.cpb_mlp.0.bias', 'layers.2.blocks.10.attn.cpb_mlp.2.weight', 'layers.2.blocks.10.attn.qkv.weight', 'layers.2.blocks.10.attn.proj.weight', 'layers.2.blocks.10.attn.proj.bias', 'layers.2.blocks.10.norm1.weight', 'layers.2.blocks.10.norm1.bias', 'layers.2.blocks.10.mlp.fc1.weight', 'layers.2.blocks.10.mlp.fc1.bias', 'layers.2.blocks.10.mlp.fc2.weight', 'layers.2.blocks.10.mlp.fc2.bias', 'layers.2.blocks.10.norm2.weight', 'layers.2.blocks.10.norm2.bias', 'layers.2.blocks.11.attn.logit_scale', 'layers.2.blocks.11.attn.q_bias', 'layers.2.blocks.11.attn.v_bias', 'layers.2.blocks.11.attn.cpb_mlp.0.weight', 'layers.2.blocks.11.attn.cpb_mlp.0.bias', 'layers.2.blocks.11.attn.cpb_mlp.2.weight', 'layers.2.blocks.11.attn.qkv.weight', 'layers.2.blocks.11.attn.proj.weight', 'layers.2.blocks.11.attn.proj.bias', 'layers.2.blocks.11.norm1.weight', 'layers.2.blocks.11.norm1.bias', 'layers.2.blocks.11.mlp.fc1.weight', 'layers.2.blocks.11.mlp.fc1.bias', 'layers.2.blocks.11.mlp.fc2.weight', 'layers.2.blocks.11.mlp.fc2.bias', 'layers.2.blocks.11.norm2.weight', 'layers.2.blocks.11.norm2.bias', 'layers.2.blocks.12.attn.logit_scale', 'layers.2.blocks.12.attn.q_bias', 'layers.2.blocks.12.attn.v_bias', 'layers.2.blocks.12.attn.cpb_mlp.0.weight', 'layers.2.blocks.12.attn.cpb_mlp.0.bias', 'layers.2.blocks.12.attn.cpb_mlp.2.weight', 'layers.2.blocks.12.attn.qkv.weight', 'layers.2.blocks.12.attn.proj.weight', 'layers.2.blocks.12.attn.proj.bias', 'layers.2.blocks.12.norm1.weight', 'layers.2.blocks.12.norm1.bias', 'layers.2.blocks.12.mlp.fc1.weight', 'layers.2.blocks.12.mlp.fc1.bias', 'layers.2.blocks.12.mlp.fc2.weight', 'layers.2.blocks.12.mlp.fc2.bias', 'layers.2.blocks.12.norm2.weight', 'layers.2.blocks.12.norm2.bias', 'layers.2.blocks.13.attn.logit_scale', 'layers.2.blocks.13.attn.q_bias', 'layers.2.blocks.13.attn.v_bias', 'layers.2.blocks.13.attn.cpb_mlp.0.weight', 'layers.2.blocks.13.attn.cpb_mlp.0.bias', 'layers.2.blocks.13.attn.cpb_mlp.2.weight', 'layers.2.blocks.13.attn.qkv.weight', 'layers.2.blocks.13.attn.proj.weight', 'layers.2.blocks.13.attn.proj.bias', 'layers.2.blocks.13.norm1.weight', 'layers.2.blocks.13.norm1.bias', 'layers.2.blocks.13.mlp.fc1.weight', 'layers.2.blocks.13.mlp.fc1.bias', 'layers.2.blocks.13.mlp.fc2.weight', 'layers.2.blocks.13.mlp.fc2.bias', 'layers.2.blocks.13.norm2.weight', 'layers.2.blocks.13.norm2.bias', 'layers.2.blocks.14.attn.logit_scale', 'layers.2.blocks.14.attn.q_bias', 'layers.2.blocks.14.attn.v_bias', 'layers.2.blocks.14.attn.cpb_mlp.0.weight', 'layers.2.blocks.14.attn.cpb_mlp.0.bias', 'layers.2.blocks.14.attn.cpb_mlp.2.weight', 'layers.2.blocks.14.attn.qkv.weight', 'layers.2.blocks.14.attn.proj.weight', 'layers.2.blocks.14.attn.proj.bias', 'layers.2.blocks.14.norm1.weight', 'layers.2.blocks.14.norm1.bias', 'layers.2.blocks.14.mlp.fc1.weight', 'layers.2.blocks.14.mlp.fc1.bias', 'layers.2.blocks.14.mlp.fc2.weight', 'layers.2.blocks.14.mlp.fc2.bias', 'layers.2.blocks.14.norm2.weight', 'layers.2.blocks.14.norm2.bias', 'layers.2.blocks.15.attn.logit_scale', 'layers.2.blocks.15.attn.q_bias', 'layers.2.blocks.15.attn.v_bias', 'layers.2.blocks.15.attn.cpb_mlp.0.weight', 'layers.2.blocks.15.attn.cpb_mlp.0.bias', 'layers.2.blocks.15.attn.cpb_mlp.2.weight', 'layers.2.blocks.15.attn.qkv.weight', 'layers.2.blocks.15.attn.proj.weight', 'layers.2.blocks.15.attn.proj.bias', 'layers.2.blocks.15.norm1.weight', 'layers.2.blocks.15.norm1.bias', 'layers.2.blocks.15.mlp.fc1.weight', 'layers.2.blocks.15.mlp.fc1.bias', 'layers.2.blocks.15.mlp.fc2.weight', 'layers.2.blocks.15.mlp.fc2.bias', 'layers.2.blocks.15.norm2.weight', 'layers.2.blocks.15.norm2.bias', 'layers.2.blocks.16.attn.logit_scale', 'layers.2.blocks.16.attn.q_bias', 'layers.2.blocks.16.attn.v_bias', 'layers.2.blocks.16.attn.cpb_mlp.0.weight', 'layers.2.blocks.16.attn.cpb_mlp.0.bias', 'layers.2.blocks.16.attn.cpb_mlp.2.weight', 'layers.2.blocks.16.attn.qkv.weight', 'layers.2.blocks.16.attn.proj.weight', 'layers.2.blocks.16.attn.proj.bias', 'layers.2.blocks.16.norm1.weight', 'layers.2.blocks.16.norm1.bias', 'layers.2.blocks.16.mlp.fc1.weight', 'layers.2.blocks.16.mlp.fc1.bias', 'layers.2.blocks.16.mlp.fc2.weight', 'layers.2.blocks.16.mlp.fc2.bias', 'layers.2.blocks.16.norm2.weight', 'layers.2.blocks.16.norm2.bias', 'layers.2.blocks.17.attn.logit_scale', 'layers.2.blocks.17.attn.q_bias', 'layers.2.blocks.17.attn.v_bias', 'layers.2.blocks.17.attn.cpb_mlp.0.weight', 'layers.2.blocks.17.attn.cpb_mlp.0.bias', 'layers.2.blocks.17.attn.cpb_mlp.2.weight', 'layers.2.blocks.17.attn.qkv.weight', 'layers.2.blocks.17.attn.proj.weight', 'layers.2.blocks.17.attn.proj.bias', 'layers.2.blocks.17.norm1.weight', 'layers.2.blocks.17.norm1.bias', 'layers.2.blocks.17.mlp.fc1.weight', 'layers.2.blocks.17.mlp.fc1.bias', 'layers.2.blocks.17.mlp.fc2.weight', 'layers.2.blocks.17.mlp.fc2.bias', 'layers.2.blocks.17.norm2.weight', 'layers.2.blocks.17.norm2.bias', 'layers.3.downsample.reduction.weight', 'layers.3.downsample.norm.weight', 'layers.3.downsample.norm.bias', 'layers.3.blocks.0.attn.logit_scale', 'layers.3.blocks.0.attn.q_bias', 'layers.3.blocks.0.attn.v_bias', 'layers.3.blocks.0.attn.cpb_mlp.0.weight', 'layers.3.blocks.0.attn.cpb_mlp.0.bias', 'layers.3.blocks.0.attn.cpb_mlp.2.weight', 'layers.3.blocks.0.attn.qkv.weight', 'layers.3.blocks.0.attn.proj.weight', 'layers.3.blocks.0.attn.proj.bias', 'layers.3.blocks.0.norm1.weight', 'layers.3.blocks.0.norm1.bias', 'layers.3.blocks.0.mlp.fc1.weight', 'layers.3.blocks.0.mlp.fc1.bias', 'layers.3.blocks.0.mlp.fc2.weight', 'layers.3.blocks.0.mlp.fc2.bias', 'layers.3.blocks.0.norm2.weight', 'layers.3.blocks.0.norm2.bias', 'layers.3.blocks.1.attn.logit_scale', 'layers.3.blocks.1.attn.q_bias', 'layers.3.blocks.1.attn.v_bias', 'layers.3.blocks.1.attn.cpb_mlp.0.weight', 'layers.3.blocks.1.attn.cpb_mlp.0.bias', 'layers.3.blocks.1.attn.cpb_mlp.2.weight', 'layers.3.blocks.1.attn.qkv.weight', 'layers.3.blocks.1.attn.proj.weight', 'layers.3.blocks.1.attn.proj.bias', 'layers.3.blocks.1.norm1.weight', 'layers.3.blocks.1.norm1.bias', 'layers.3.blocks.1.mlp.fc1.weight', 'layers.3.blocks.1.mlp.fc1.bias', 'layers.3.blocks.1.mlp.fc2.weight', 'layers.3.blocks.1.mlp.fc2.bias', 'layers.3.blocks.1.norm2.weight', 'layers.3.blocks.1.norm2.bias', 'norm.weight', 'norm.bias', 'head.fc.weight', 'head.fc.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"swin_state_dictV2:\",swin_state_dictV2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in new_swin_state_dictV2:\n",
    "              \n",
    "                if (\n",
    "                    x.endswith(\"relative_position_bias_table\")\n",
    "                    and modelV2.layers[0].blocks[0].attn.window_size[0] != 12\n",
    "                ):\n",
    "                    print(x)\n",
    "                    \n",
    "                    pos_bias = swin_state_dictV2[x].unsqueeze(0)[0]\n",
    "                    old_len = int(math.sqrt(len(pos_bias)))\n",
    "                    new_len = int(2 * window_size - 1)\n",
    "                    pos_bias = pos_bias.reshape(1, old_len, old_len, -1).permute(\n",
    "                        0, 3, 1, 2\n",
    "                    )\n",
    "                    pos_bias = F.interpolate(\n",
    "                        pos_bias,\n",
    "                        size=(new_len, new_len),\n",
    "                        mode=\"bicubic\",\n",
    "                        align_corners=False,\n",
    "                    )\n",
    "                    new_swin_state_dictV2[x] = (\n",
    "                        pos_bias.permute(0, 2, 3, 1)\n",
    "                        .reshape(1, new_len**2, -1)\n",
    "                        .squeeze(0)\n",
    "                    )\n",
    "                else:\n",
    "                    new_swin_state_dictV2[x] = swin_state_dict[x]\n",
    "            # modelV2.load_state_dict(new_swin_state_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
